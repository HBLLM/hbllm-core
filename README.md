<div align="center">
  <h1>ğŸ§  HBLLM Core</h1>
  <p><b>Human-Brain Inspired Cognitive Architecture</b></p>
  <p><em>An open-source AGI framework that thinks, learns, and adapts â€” not just responds.</em></p>

  [![Python 3.11+](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/)
  [![PyTorch](https://img.shields.io/badge/PyTorch-2.2%2B-ee4c2c.svg)](https://pytorch.org/)
  [![Rust](https://img.shields.io/badge/Rust-Accelerated-orange.svg)](https://www.rust-lang.org/)
  [![Tests](https://img.shields.io/badge/Tests-529%2B%20passing-brightgreen.svg)](#)
  [![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
</div>

<br/>

## What Makes This Different

**Standard LLMs** are monolithic transformers: prompt â†’ model â†’ response. One path, one perspective, stateless.

**HBLLM Core** is a **modular cognitive architecture** with 25 specialized brain nodes that communicate over an asynchronous message bus â€” like a real brain:

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚              HBLLM Core Brain           â”‚
    (text, vision,      â”‚                                         â”‚
     audio, sensors)    â”‚   Router â”€â”€â–º Planner â”€â”€â–º Decision       â”‚
                        â”‚     â”‚          â”‚            â”‚           â”‚
                        â”‚   Memory    Learner      Critic        â”‚
                        â”‚   (5 types)    â”‚            â”‚           â”‚
                        â”‚              World       Identity      â”‚
                        â”‚              Model       (ethics)      â”‚
                        â”‚                â”‚                        â”‚
                        â”‚           Curiosity â”€â”€â–º Spawner        â”‚
                        â”‚           (explores)    (creates new   â”‚
                        â”‚                          specialists)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
    Output â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    (actions, speech, motor control, API calls)
```

## ğŸ”§ The Zoning Model â€” Why We Don't Need 70B Parameters

Most AI companies chase **bigger models** â€” 70B, 405B, even trillion-parameter behemoths. HBLLM takes the opposite approach: **small, specialized models working together in zones**, like the human brain.

### The Problem with Monolithic LLMs

```
Traditional Approach:                    HBLLM Zoning Approach:
                                         
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚                      â”‚                 â”‚ 125M â”‚ â”‚ 125M â”‚ â”‚ 125M â”‚
â”‚   ONE MASSIVE MODEL  â”‚                 â”‚Generalâ”‚ â”‚Codingâ”‚ â”‚ Math â”‚
â”‚     70B+ params      â”‚                 â”‚  +    â”‚ â”‚  +   â”‚ â”‚  +   â”‚
â”‚                      â”‚                 â”‚ LoRA  â”‚ â”‚ LoRA â”‚ â”‚ LoRA â”‚
â”‚  Knows everything    â”‚                 â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
â”‚  but not deeply      â”‚                    â”‚        â”‚        â”‚
â”‚                      â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  Costs $$$$ to run   â”‚                         â”‚
â”‚  Needs 80GB GPU      â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      â”‚                 â”‚ Shared Base LLM â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   125M-1.5B     â”‚
                                         â”‚ Runs on CPU/Pi  â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How Zoning Works

HBLLM uses **one small base model** (125M, 500M, or 1.5B parameters) with **lightweight LoRA adapters** that hot-swap depending on the task:

| Component           | Size        | Purpose                                             |
| ------------------- | ----------- | --------------------------------------------------- |
| **Base Model**      | 125Mâ€“1.5B   | Shared transformer backbone (GQA + SwiGLU + RoPE)   |
| **LoRA Adapters**   | ~2MB each   | Domain specialization (General, Coding, Math, ...)  |
| **MoE Router**      | Tiny        | Routes to the right expert automatically            |
| **Cognitive Nodes** | Zero params | Orchestration, planning, memory â€” no weights needed |

```python
# How it works internally:
#
# 1. Query arrives: "Write a sorting algorithm"
# 2. Router Node identifies: this is a CODING task
# 3. Base model loads the Coding LoRA adapter (2MB swap, <1ms)
# 4. Coding expert generates the response
# 5. Critic Node evaluates quality
# 6. If quality < threshold â†’ try Math expert too (ensemble)
# 7. Decision Node picks the best answer
```

### ğŸ§¬ Self-Expanding Zones â€” The Brain Grows Itself

This is the part that no other framework does. **Zones are NOT hardcoded.** HBLLM ships with 3 starter zones (General, Coding, Math), but the **SpawnerNode** creates new specialists automatically when the Router encounters an unfamiliar domain:

```
User asks: "What's the best soil pH for tomatoes?"

Router: "I don't have a GARDENING zone..."
   â”‚
   â–¼
SpawnerNode activates:
   â”‚
   â”œâ”€â”€ 1. Generates synthetic training data for "gardening"
   â”‚
   â”œâ”€â”€ 2. Trains a new LoRA adapter (~2MB, takes seconds)
   â”‚
   â”œâ”€â”€ 3. Creates DomainModuleNode("domain_gardening")
   â”‚
   â”œâ”€â”€ 4. Wires it to the live MessageBus
   â”‚
   â””â”€â”€ 5. Announces SPAWN_COMPLETE â†’ new zone is active!
       
Next gardening question â†’ routes directly to the new expert.
The brain literally grew a new region. ğŸ§ 
```

```python
# This happens automatically â€” no restart, no redeployment:
#
# Boot:     [General] [Coding] [Math]           â† 3 starter zones
# Week 1:   + [Gardening] + [Cooking]           â† spawned on demand
# Week 4:   + [Home_Automation] + [IoT]         â† from smart home usage
# Month 2:  + [Energy_Management] + [Security]  â† deeper specialization
#
# Each new zone is a 2MB LoRA adapter on the shared base model.
# Total memory: base model + (N Ã— 2MB) â€” scales to hundreds of zones.
```

This is **artificial neurogenesis** â€” the same process biological brains use to grow new neural pathways for new skills.

### Why This Matters

| Metric             | GPT-4 / Claude      | HBLLM Zoning                 |
| ------------------ | ------------------- | ---------------------------- |
| **Parameters**     | 70Bâ€“1.7T            | 125Mâ€“1.5B                    |
| **GPU Required**   | 80GB A100           | CPU / 4GB GPU / Raspberry Pi |
| **Cost per Query** | $0.01â€“0.06          | $0.0001 (local)              |
| **Domain Depth**   | Generalist          | Deep specialist per zone     |
| **Add New Domain** | Retrain everything  | Auto-spawns in seconds       |
| **Privacy**        | Cloud-only          | 100% on-device               |
| **Latency**        | 200â€“2000ms (API)    | 10â€“50ms (local)              |
| **Self-Expanding** | âŒ Fixed at training | âœ… Grows new zones at runtime |

### Model Presets

| Preset | Params | Layers | Heads | Context | Best For                  |
| ------ | ------ | ------ | ----- | ------- | ------------------------- |
| `125M` | ~125M  | 12     | 12    | 2048    | Edge / Raspberry Pi / IoT |
| `500M` | ~500M  | 24     | 16    | 4096    | Desktop / Home Server     |
| `1.5B` | ~1.5B  | 32     | 32    | 4096    | Workstation / GPU         |

### Built-in MoE (Mixture of Experts)

For advanced deployments, HBLLM supports **Mixture of Experts** â€” 16 micro-experts with only 2 active per token:

```yaml
# config/model.yaml
use_moe: true
num_experts: 16        # Total specialist micro-experts
num_active_experts: 2  # Only 2 fire per token (efficient!)
use_shared_expert: true # One expert always active (stability)
```

This means a 1.5B MoE model has the **capacity of a much larger model** but the **compute cost of a small one**.

### The Key Insight

> **Intelligence isn't about having the biggest brain. It's about having the right specialists â€” and growing new ones when you need them.**
>
> A 125M base model + self-expanding LoRA zones + cognitive nodes = an AI that **gets smarter the more you use it**, on hardware you already own.

---

## Architecture

### Full System Overview

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                            HBLLM CORE BRAIN                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                            â•‘
â•‘  â”Œâ”€â”€â”€ PERCEPTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ACTIONS â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                                                                      â”‚  â•‘
â•‘  â”‚  ğŸ‘ï¸ Vision    ğŸ¤ Audio In    ğŸ”Š Audio Out    âš¡ Execution  ğŸŒ API   â”‚  â•‘
â•‘  â”‚                                               ğŸ–¥ï¸ Browser  ğŸ”§ Logic  â”‚  â•‘
â•‘  â”‚                                               ğŸŒ€ Fuzzy    ğŸ”Œ MCP    â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘             â”‚                                               â”‚              â•‘
â•‘             â–¼                                               â–²              â•‘
â•‘  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â•‘
â•‘  â•‘                     âš¡ MESSAGE BUS (Async Pub/Sub)                  â•‘   â•‘
â•‘  â•‘              Service Registry â”‚ Circuit Breaker â”‚ Tracing           â•‘   â•‘
â•‘  â•šâ•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â•‘
â•‘          â”‚           â”‚        â”‚          â”‚            â”‚                    â•‘
â•‘          â–¼           â–¼        â–¼          â–¼            â–¼                    â•‘
â•‘  â”Œâ”€â”€â”€ BRAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚                                                                    â”‚   â•‘
â•‘  â”‚  ğŸ”€ Router â”€â”€â”€â”€â”€â”€â–º ğŸ“‹ Planner â”€â”€â”€â”€â”€â”€â–º âš–ï¸ Decision                 â”‚   â•‘
â•‘  â”‚     â”‚                  â”‚                    â”‚                       â”‚   â•‘
â•‘  â”‚     â”‚              ğŸ“ Learner           ğŸ” Critic                  â”‚   â•‘
â•‘  â”‚     â”‚                  â”‚                    â”‚                       â”‚   â•‘
â•‘  â”‚     â”‚              ğŸŒ World Model      ğŸ›¡ï¸ Identity (Ethics)       â”‚   â•‘
â•‘  â”‚     â”‚                  â”‚                                           â”‚   â•‘
â•‘  â”‚     â”‚              ğŸ”­ Curiosity â”€â”€â–º ğŸ§¬ Spawner (Neurogenesis)     â”‚   â•‘
â•‘  â”‚     â”‚                                   â”‚                          â”‚   â•‘
â•‘  â”‚     â”‚              ğŸ§  Meta Reasoning    â”‚ creates new zones        â”‚   â•‘
â•‘  â”‚     â”‚              ğŸ“Š Collective        â–¼ at runtime!              â”‚   â•‘
â•‘  â”‚     â”‚              ğŸ’¤ Sleep Cycle    [New Domain Experts...]        â”‚   â•‘
â•‘  â”‚     â”‚              ğŸ“ Workspace                                    â”‚   â•‘
â•‘  â”‚     â”‚                                                              â”‚   â•‘
â•‘  â”‚     â”‚  â”Œâ”€â”€ POLICY ENGINE (YAML governance rules) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â•‘
â•‘  â”‚     â””â”€â–ºâ”‚ Safety constraints â€¢ Rate limits â€¢ Content filtering  â”‚   â”‚   â•‘
â•‘  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘          â”‚                                                                â•‘
â•‘          â–¼                                                                â•‘
â•‘  â”Œâ”€â”€â”€ MEMORY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚                                                                    â”‚   â•‘
â•‘  â”‚  ğŸ“– Episodic     ğŸ“š Semantic    ğŸ”§ Procedural    â¤ï¸ Value         â”‚   â•‘
â•‘  â”‚  (events)        (facts)        (skills)          (preferences)    â”‚   â•‘
â•‘  â”‚                                                                    â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                                           â•‘
â•‘  â”Œâ”€â”€â”€ ZONING MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚                                                                    â”‚   â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚   â•‘
â•‘  â”‚  â”‚ General â”‚  â”‚ Coding  â”‚  â”‚  Math   â”‚  â”‚  ???    â”‚  â”‚  ...  â”‚  â”‚   â•‘
â•‘  â”‚  â”‚  LoRA   â”‚  â”‚  LoRA   â”‚  â”‚  LoRA   â”‚  â”‚  LoRA   â”‚  â”‚ LoRA  â”‚  â”‚   â•‘
â•‘  â”‚  â”‚  ~2MB   â”‚  â”‚  ~2MB   â”‚  â”‚  ~2MB   â”‚  â”‚  ~2MB   â”‚  â”‚ ~2MB  â”‚  â”‚   â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚   â•‘
â•‘  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â•‘
â•‘  â”‚                             â”‚                                     â”‚   â•‘
â•‘  â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚   â•‘
â•‘  â”‚                  â”‚  Shared Base Model  â”‚                          â”‚   â•‘
â•‘  â”‚                  â”‚  125M / 500M / 1.5B â”‚                          â”‚   â•‘
â•‘  â”‚                  â”‚  GQA + SwiGLU + RoPEâ”‚                          â”‚   â•‘
â•‘  â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚   â•‘
â•‘  â”‚       â–² starter zones        â–² spawned dynamically               â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ğŸ§  Brain Nodes (13 cognitive modules + growing)

| Node               | Role                                       | Analog               |
| ------------------ | ------------------------------------------ | -------------------- |
| **Router**         | Routes inputs to the right cognitive path  | Thalamus             |
| **Planner**        | Breaks goals into multi-step plans         | Prefrontal cortex    |
| **Decision**       | Makes final decisions from evidence        | Executive function   |
| **Critic**         | Self-evaluates quality and correctness     | Error monitoring     |
| **Learner**        | Updates knowledge from outcomes            | Hippocampal learning |
| **Curiosity**      | Explores novel situations proactively      | Dopaminergic system  |
| **World Model**    | Builds internal model of the environment   | Predictive coding    |
| **Identity**       | Maintains values, ethics, and personality  | Self-model           |
| **Meta Reasoning** | Reasons about its own reasoning            | Metacognition        |
| **Workspace**      | Shared cognitive workspace for integration | Global workspace     |
| **Collective**     | Ensemble reasoning from multiple nodes     | Neural ensemble      |
| **Sleep Cycle**    | Consolidates learning during idle time     | Memory consolidation |
| **Spawner**        | Dynamically creates specialist sub-agents  | Neurogenesis         |

### ğŸ‘ï¸ Perception (3 input channels)

| Node             | Capability                                |
| ---------------- | ----------------------------------------- |
| **Vision**       | Image understanding and visual processing |
| **Audio Input**  | Speech recognition and sound analysis     |
| **Audio Output** | Speech synthesis and audio generation     |

### ğŸ§¬ Memory Systems (5 types â€” like human memory)

| Memory         | What It Stores            | Example                                     |
| -------------- | ------------------------- | ------------------------------------------- |
| **Episodic**   | Events and experiences    | "User came home at 6:30pm on Tuesday"       |
| **Semantic**   | Facts and knowledge       | "Living room temperature preference = 23Â°C" |
| **Procedural** | Skills and how-to         | "To make coffee: fill water â†’ grind â†’ brew" |
| **Value**      | Preferences and judgments | "User prefers warm lighting over cool"      |
| **Working**    | Current task context      | Active conversation state                   |

### âš¡ Action Nodes (8 output channels)

| Node           | Capability                           | Optional Dependency |
| -------------- | ------------------------------------ | ------------------- |
| **Execution**  | Run tasks and commands               | â€”                   |
| **API**        | Call external APIs and services      | â€”                   |
| **Browser**    | Web interaction and scraping         | playwright          |
| **Logic**      | Formal logical reasoning (Z3 solver) | z3-solver           |
| **Fuzzy**      | Probabilistic/fuzzy reasoning        | â€”                   |
| **MCP Client** | Model Context Protocol integration   | â€”                   |
| **IoT/MQTT**   | Home automation device control       | paho-mqtt           |
| **ROS2**       | Robotics (Nav2, MoveIt2, sensors)    | rclpy               |

### ğŸ”Œ Infrastructure

| Component            | Purpose                                       |
| -------------------- | --------------------------------------------- |
| **MessageBus**       | Async pub/sub communication between all nodes |
| **Service Registry** | Dynamic node discovery and routing            |
| **Circuit Breaker**  | Fault tolerance and graceful degradation      |
| **Load Balancer**    | Distribute work across node replicas          |
| **Policy Engine**    | YAML-based governance rules                   |
| **Tracing**          | Full observability of cognitive processing    |

---

## Use Cases

### ğŸ  Smart Home & Home Automation

HBLLM Core can power truly intelligent home systems that **learn and adapt** â€” not just follow rules:

```python
from hbllm.network.bus import InProcessBus
from hbllm.brain.router_node import RouterNode
from hbllm.brain.planner_node import PlannerNode
from hbllm.brain.decision_node import DecisionNode
from hbllm.brain.learner_node import LearnerNode
from hbllm.brain.world_model_node import WorldModelNode
from hbllm.memory.memory_node import MemoryNode

# The brain learns your patterns over time:
#
# Week 1:  "Turn on lights" â†’ turns on lights
# Week 4:  Notices you always dim lights at 9pm â†’ does it automatically  
# Month 2: Learns your wake-up routine â†’ starts coffee before alarm
# Month 6: Predicts energy usage â†’ optimizes heating/cooling schedule
```

**What makes it different from Google Home / Alexa:**

| Feature    | Alexa/Google            | HBLLM Core                              |
| ---------- | ----------------------- | --------------------------------------- |
| Learning   | Pre-programmed routines | Learns from observation                 |
| Memory     | Stateless commands      | Episodic + semantic + procedural memory |
| Planning   | Single-step actions     | Multi-step plans (Planner node)         |
| Adaptation | Manual rule updates     | Self-improving (Learner node)           |
| Privacy    | Cloud-dependent         | Runs 100% locally                       |
| Reasoning  | Pattern matching        | Logical + fuzzy + world model           |

### ğŸ¤– Robotics

HBLLM Core provides the cognitive layer for autonomous robots:

```
Sensors â”€â”€â–º Perception Nodes â”€â”€â–º Router â”€â”€â–º Planner â”€â”€â–º Decision â”€â”€â–º Motors
  â”‚                                â”‚                        â”‚
  â”‚                          World Model              Critic
  â”‚                        (understands               (checks
  â”‚                         physics,                   safety)
  â”‚                         obstacles)                   â”‚
  â””â”€â”€â”€â”€â”€â”€ Memory â—„â”€â”€â”€â”€â”€â”€â”€â”€ Learner â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         (remembers         (improves
          what worked)       from mistakes)
```

**Capabilities:**
- **Path planning** â€” Planner node breaks navigation into steps
- **Object manipulation** â€” World Model understands physical constraints  
- **Failure recovery** â€” Critic detects errors, Learner adapts
- **Task learning** â€” Procedural memory stores learned skills
- **Human interaction** â€” Audio + Vision perception for natural communication
- **Safety** â€” Policy engine enforces hard safety constraints

### ğŸ­ Industrial Automation

| Application                  | How HBLLM Core Helps                                     |
| ---------------------------- | -------------------------------------------------------- |
| **Predictive maintenance**   | World Model learns equipment patterns, predicts failures |
| **Quality control**          | Vision node + Critic node detect defects and anomalies   |
| **Process optimization**     | Learner node continuously improves production parameters |
| **Multi-robot coordination** | Message bus enables distributed swarm intelligence       |

### ğŸ§ª Research & General AI

- **Cognitive science** â€” Experiment with different brain architectures
- **Reinforcement learning** â€” Built-in reward/feedback loops
- **Multi-agent systems** â€” Spawner creates specialized sub-agents
- **Embodied AI** â€” Connect perception and action for physical agents

---

## Quick Start

### Installation

```bash
pip install -e .

# Optional dependencies (only needed if you use these features):
pip install paho-mqtt        # For IoT/MQTT home automation
# ROS2: install rclpy from your ROS2 distribution, then:
export HBLLM_ROS2_ENABLED=1  # Enable real robot control
```

### CLI

```bash
hbllm info               # Show architecture summary
hbllm nodes              # List all 25 brain nodes
hbllm serve              # Start the API server
hbllm serve --port 9000  # Custom port
hbllm data --dataset fineweb --samples 100000  # Data pipeline
hbllm train --model-size 125m                  # Training
```

### Run the API Server

```bash
# Full brain mode (requires model weights)
python -m hbllm.serving.api

# Provider mode (uses OpenAI/Anthropic as backend)
HBLLM_PROVIDER=openai OPENAI_API_KEY=sk-... python -m hbllm.serving.api
```

### Run Benchmarks

```bash
python -m hbllm.benchmarks.runner --suite all          # All 4 suites
python -m hbllm.benchmarks.runner --suite latency       # Bus latency
python -m hbllm.benchmarks.runner --suite memory        # LoRA vs monolithic
python -m hbllm.benchmarks.runner --suite multi_tenant  # Tenant isolation
python -m hbllm.benchmarks.runner --output results.json # Save JSON
```

### Python API

```python
import asyncio
from hbllm.network.bus import InProcessBus
from hbllm.brain.router_node import RouterNode
from hbllm.brain.decision_node import DecisionNode
from hbllm.memory.memory_node import MemoryNode
from hbllm.network.messages import Message, MessageType

async def main():
    bus = InProcessBus()
    await bus.start()

    # Start cognitive nodes
    memory = MemoryNode(node_id="memory_01", db_path="brain.db")
    router = RouterNode(node_id="router_01")
    decision = DecisionNode(node_id="decision_01")

    for node in [memory, router, decision]:
        await node.start(bus)

    # Send a message through the cognitive pipeline
    msg = Message(
        type=MessageType.QUERY,
        source_node_id="user",
        topic="router.query",
        payload={"text": "What's the optimal temperature for my living room?"},
    )
    await bus.publish("router.query", msg)

asyncio.run(main())
```

---

## Project Structure

```
hbllm-core/
â”œâ”€â”€ hbllm/                    # Python cognitive architecture
â”‚   â”œâ”€â”€ brain/                # 13 cognitive nodes
â”‚   â”‚   â”œâ”€â”€ router_node.py    #   Input routing (thalamus)
â”‚   â”‚   â”œâ”€â”€ planner_node.py   #   Multi-step planning
â”‚   â”‚   â”œâ”€â”€ decision_node.py  #   Final decision making
â”‚   â”‚   â”œâ”€â”€ critic_node.py    #   Self-evaluation
â”‚   â”‚   â”œâ”€â”€ learner_node.py   #   Learning from outcomes
â”‚   â”‚   â”œâ”€â”€ curiosity_node.py #   Exploration drive
â”‚   â”‚   â”œâ”€â”€ world_model_node.py # Environment modeling
â”‚   â”‚   â”œâ”€â”€ identity_node.py  #   Values and ethics
â”‚   â”‚   â”œâ”€â”€ meta_node.py      #   Meta-reasoning
â”‚   â”‚   â”œâ”€â”€ workspace_node.py #   Cognitive workspace
â”‚   â”‚   â”œâ”€â”€ collective_node.py#   Ensemble reasoning
â”‚   â”‚   â”œâ”€â”€ sleep_node.py     #   Memory consolidation
â”‚   â”‚   â”œâ”€â”€ spawner_node.py   #   Dynamic agent creation
â”‚   â”‚   â”œâ”€â”€ policy_engine.py  #   Governance rules
â”‚   â”‚   â””â”€â”€ llm_interface.py  #   Model abstraction
â”‚   â”œâ”€â”€ memory/               # 5 memory systems
â”‚   â”‚   â”œâ”€â”€ episodic.py       #   Event memory
â”‚   â”‚   â”œâ”€â”€ semantic.py       #   Fact memory
â”‚   â”‚   â”œâ”€â”€ procedural.py     #   Skill memory
â”‚   â”‚   â””â”€â”€ value_memory.py   #   Preference memory
â”‚   â”œâ”€â”€ perception/           # Input channels
â”‚   â”‚   â”œâ”€â”€ vision_node.py
â”‚   â”‚   â”œâ”€â”€ audio_in_node.py
â”‚   â”‚   â””â”€â”€ audio_out_node.py
â”‚   â”œâ”€â”€ actions/              # Output channels
â”‚   â”‚   â”œâ”€â”€ execution_node.py
â”‚   â”‚   â”œâ”€â”€ api_node.py
â”‚   â”‚   â”œâ”€â”€ browser_node.py
â”‚   â”‚   â”œâ”€â”€ logic_node.py
â”‚   â”‚   â”œâ”€â”€ fuzzy_node.py
â”‚   â”‚   â”œâ”€â”€ mcp_client_node.py
â”‚   â”‚   â”œâ”€â”€ iot_mqtt_node.py  #   Home automation (optional paho-mqtt)
â”‚   â”‚   â””â”€â”€ ros2_node.py      #   Robotics (optional rclpy)
â”‚   â”œâ”€â”€ network/              # Communication infrastructure
â”‚   â”‚   â”œâ”€â”€ bus.py            #   Message bus (pub/sub)
â”‚   â”‚   â”œâ”€â”€ node.py           #   Base node abstraction
â”‚   â”‚   â”œâ”€â”€ registry.py       #   Service discovery
â”‚   â”‚   â”œâ”€â”€ circuit_breaker.py#   Fault tolerance
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ model/                # Transformer model
â”‚   â”œâ”€â”€ training/             # SFT, DPO, evaluation
â”‚   â”œâ”€â”€ benchmarks/           # Cognitive arch vs monolithic benchmarks
â”‚   â”‚   â””â”€â”€ runner.py         #   4 suites: latency, memory, routing, MT
â”‚   â””â”€â”€ serving/              # FastAPI server
â”œâ”€â”€ rust/                     # Rust accelerators
â”‚   â”œâ”€â”€ tokenizer/            #   High-performance tokenizer
â”‚   â””â”€â”€ data_tools/           #   Data cleaning & dedup
â”œâ”€â”€ tests/                    # 529+ tests
â””â”€â”€ pyproject.toml
```

---

## Extending for Your Use Case

### Adding a Custom Node

```python
from hbllm.network.node import Node, NodeType

class TemperatureSensorNode(Node):
    """Custom perception node for IoT temperature sensors."""

    def __init__(self, node_id: str, mqtt_topic: str):
        super().__init__(node_id, NodeType.DETECTOR, capabilities=["temperature"])
        self.mqtt_topic = mqtt_topic

    async def on_start(self):
        await self.bus.subscribe(self.mqtt_topic, self.handle_message)

    async def on_stop(self):
        pass

    async def handle_message(self, message):
        temp = message.payload.get("temperature")
        # Publish to the cognitive pipeline
        await self.publish("perception.temperature", Message(
            type=MessageType.EVENT,
            source_node_id=self.node_id,
            payload={"temperature": temp, "unit": "celsius"},
        ))
        return None
```

### Connecting to Hardware (Raspberry Pi / Jetson)

```python
# HBLLM runs on any Python 3.11+ system
# No GPU required in provider mode

# On Raspberry Pi:
pip install -e .
HBLLM_PROVIDER=openai python -m hbllm.serving.api --host 0.0.0.0 --port 8000
```

---

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

Key areas where we need help:
- ğŸ§  **New cognitive nodes** â€” Emotion modeling, spatial reasoning, temporal reasoning
- ğŸ“± **Edge optimization** â€” Running efficiently on Raspberry Pi / Jetson
- ğŸ  **IoT integrations** â€” Extend MQTT node with new device protocols
- ğŸ¤– **ROS2 packages** â€” Navigation planners, manipulation skills
- ğŸŒ **New LoRA domains** â€” Medical, legal, creative writing specialists

## License

MIT License â€” free to use in personal, commercial, and research projects.

---

<div align="center">
  <p><b>HBLLM Core</b> â€” AI that thinks, not just responds.</p>
  <p>â­ Star this repo if you believe AI should be more than a chatbot.</p>
</div>
